
* Tokenization</br>

1) word tokenize: NLTK word_tokenize, WordPunctTokenizer,TreebankWordTokenizer(Standard) Keras text_to_word_sequence</br>
2) sentence tokenize: NLTK sent_tokenize</br>
3) Part-of-speech tagging: NLTK pos_tag</br>
4) Morpheme tokenize (For Korean): Konlpy Okt, Mecab, Komoran, Hannanum, Kkma</br>
 
* Cleaning </br>

1) Integration of words with different notation based on rules</br>
2) Integration of upper and lower case letters</br>
3) Removal of unnecessary words (words that appear less frequently,</br> 
   words of shorter length)</br>
4) Regular expression</br>
