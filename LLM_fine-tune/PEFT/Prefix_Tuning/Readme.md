

## Pre-fix Tuning

During the fine-tune,</br> 
The model should update the entire parameters. It is getting inefficient because LLMs are becoming huge like 36B parameters.</br>
It definitely will be time consuming so they encourage us to use only 0.1% parameters instead using Prefix Tuning.</br>
It will use only 0.1% of entire LLM's parameters.</br>
