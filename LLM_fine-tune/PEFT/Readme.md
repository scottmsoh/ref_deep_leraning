

## Parameter-Efficient Fine-Tuning (PEFT)
Ref: https://github.com/huggingface/peft</br>
Methods enable efficient adaption of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters.</br>

1. LoRA (Low-Rank Adaptation of Large Language Models)</br>
   ref: [arxiv.org/abs/2106.09685](https://arxiv.org/pdf/2106.09685)</br>

2. Prefix Tuning</br>
   ref: https://arxiv.org/pdf/2101.00190</br>
   huggingface: https://huggingface.co/docs/peft/en/package_reference/prefix_tuning</br>

3. P-Tuning</br>
   ref: https://arxiv.org/pdf/2103.10385</br>
   higgingface: https://huggingface.co/docs/peft/en/package_reference/p_tuning</br>
   
4. Prompt-Tuning</br>
   ref: https://arxiv.org/pdf/2104.08691 </br>
   huggingface: https://huggingface.co/docs/peft/en/package_reference/prompt_tuning</br>

5. QLoRA</br>
   ref: </br>
   huggingface: </br>
   
6. AdaLoRA</br>
   ref: </br>
   huggingface: </br>
   
7. MultiTask Prompt Tuning</br>
   ref: </br>
   huggingface: </br>
   
8. LoHa</br>
   ref: </br>
   huggingface: </br>
   
