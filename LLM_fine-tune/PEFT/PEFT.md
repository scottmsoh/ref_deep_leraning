

## Parameter-Efficient Fine-Tuning (PEFT)
Ref: https://github.com/huggingface/peft</br>
Methods enable efficient adaption of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters.</br>

1. LoRA (Low-Rank Adaptation of Large Language Models)
2. Prefix Tuning (
3. P-Tuning
4. 
