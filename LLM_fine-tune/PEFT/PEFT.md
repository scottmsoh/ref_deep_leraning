

## Parameter-Efficient Fine-Tuning (PEFT)
Ref: https://github.com/huggingface/peft</br>
Methods enable efficient adaption of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters.</br>

1. LoRA (Low-Rank Adaptation of Large Language Models)
   ref: [arxiv.org/abs/2106.09685](https://arxiv.org/pdf/2106.09685)</br>
3. Prefix Tuning (
   ref: 
5. P-Tuning
6. Prompt-Tuning
