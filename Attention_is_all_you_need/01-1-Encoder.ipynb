{"cells":[{"cell_type":"markdown","metadata":{"id":"d5dqssavy-eb"},"source":["# Transformer Encoder Based Sentiment Analysis\n","\n","This notebook implements a Transformer-based model for sentiment analysis using the SST-2 (Stanford Sentiment Treebank) dataset. We'll go through each component of the model, the data preparation process, and the training procedure.\n"]},{"cell_type":"markdown","metadata":{"id":"aXPr425EzCRB"},"source":["## Importing Required Libraries\n","\n","These libraries provide the necessary tools for building and training our model:\n","- `torch`: The main PyTorch library for tensor computations and neural networks.\n","- `torch.nn` and `torch.nn.functional`: Contain neural network layers and activation functions.\n","- `math`: Used for mathematical operations.\n","- `transformers`: Hugging Face's library for working with transformer models.\n","- `datasets`: Hugging Face's library for easily loading and processing datasets.\n","- `DataLoader`: PyTorch's data loading utility.\n","- `tqdm`: Used for progress bars during training."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":466,"status":"ok","timestamp":1720265917212,"user":{"displayName":"SY K","userId":"03708459796910042589"},"user_tz":-540},"id":"BVE3bSnozK_x"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","from transformers import AutoTokenizer\n","from datasets import load_dataset\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{"id":"tM_re011zPGn"},"source":["## Self-Attention Mechanism\n","\n","This class implements the core self-attention mechanism:\n","1. It computes attention scores by multiplying Q (query) with K (key).\n","2. The scores are scaled by the square root of the dimension of the keys.\n","3. Softmax is applied to get attention weights.\n","4. The weights are used to compute a weighted sum of V (values)."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1720265917836,"user":{"displayName":"SY K","userId":"03708459796910042589"},"user_tz":-540},"id":"dw5WIcjrzRa5"},"outputs":[],"source":["class SelfAttention(nn.Module):\n","    def __init__(self, d_k):\n","        super().__init__()\n","        self.d_k = d_k\n","\n","    def forward(self, Q, K, V):\n","        # Q, K, V shape: (batch_size, seq_len, d_k)\n","\n","        # Compute attention scores\n","        # (batch_size, seq_len, d_k) @ (batch_size, d_k, seq_len) -> (batch_size, seq_len, seq_len)\n","        attn_scores = torch.bmm(Q, K.transpose(1, 2))\n","\n","        # Scale attention scores\n","        attn_scores = attn_scores / math.sqrt(self.d_k)\n","\n","        # Compute attention weights\n","        # (batch_size, seq_len, seq_len)\n","        attn_weights = F.softmax(attn_scores, dim=-1)\n","\n","        # Apply attention weights to values\n","        # (batch_size, seq_len, seq_len) @ (batch_size, seq_len, d_k) -> (batch_size, seq_len, d_k)\n","        output = torch.bmm(attn_weights, V)\n","\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"NInb8tS0zahI"},"source":["## Multi-Head Attention\n","\n","This class implements multi-head attention:\n","1. It creates separate projections for Q, K, and V.\n","2. The input is split into multiple heads.\n","3. Self-attention is applied to each head separately.\n","4. The results from all heads are concatenated and passed through a final linear layer."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1720265917836,"user":{"displayName":"SY K","userId":"03708459796910042589"},"user_tz":-540},"id":"Ptxu7fRWzZVX"},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, h):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.h = h\n","        self.d_k = d_model // h\n","\n","        self.W_Q = nn.Linear(d_model, d_model)\n","        self.W_K = nn.Linear(d_model, d_model)\n","        self.W_V = nn.Linear(d_model, d_model)\n","        self.W_O = nn.Linear(d_model, d_model)\n","\n","        self.attention = SelfAttention(self.d_k)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, seq_len, d_model)\n","        batch_size, seq_len, _ = x.size()\n","\n","        # Linear transformations and split into h heads\n","        # (batch_size, seq_len, d_model) -> (batch_size, h, seq_len, d_k)\n","        Q = self.W_Q(x).view(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n","        K = self.W_K(x).view(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n","        V = self.W_V(x).view(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n","\n","        # Apply attention to each head\n","        # (batch_size, h, seq_len, d_k)\n","        attn_outputs = []\n","        for i in range(self.h):\n","            attn_output = self.attention(Q[:, i], K[:, i], V[:, i])\n","            attn_outputs.append(attn_output)\n","\n","        # Concatenate attention outputs from all heads\n","        # (batch_size, seq_len, d_model)\n","        attn_output = torch.cat(attn_outputs, dim=-1)\n","\n","        # Final linear transformation\n","        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n","        output = self.W_O(attn_output)\n","\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"lKupB8qOzZGm"},"source":["## Transformer Block\n","\n","This class represents a single Transformer block:\n","1. It applies multi-head self-attention to the input.\n","2. The output is passed through a feed-forward neural network.\n","3. Layer normalization and residual connections are used after each sub-layer."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1720265917837,"user":{"displayName":"SY K","userId":"03708459796910042589"},"user_tz":-540},"id":"mYF0M4QlzgfR"},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, h):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.h = h\n","        self.d_k = d_model // h\n","\n","        self.W_Q = nn.Linear(d_model, d_model)\n","        self.W_K = nn.Linear(d_model, d_model)\n","        self.W_V = nn.Linear(d_model, d_model)\n","        self.W_O = nn.Linear(d_model, d_model)\n","\n","        self.attention = SelfAttention(self.d_k)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, seq_len, d_model)\n","        batch_size, seq_len, _ = x.size()\n","\n","        # Linear transformations and split into h heads\n","        # (batch_size, seq_len, d_model) -> (batch_size, h, seq_len, d_k)\n","        Q = self.W_Q(x).view(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n","        K = self.W_K(x).view(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n","        V = self.W_V(x).view(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n","\n","        # Apply attention to each head\n","        # (batch_size, h, seq_len, d_k)\n","        attn_outputs = []\n","        for i in range(self.h):\n","            attn_output = self.attention(Q[:, i], K[:, i], V[:, i])\n","            attn_outputs.append(attn_output)\n","\n","        # Concatenate attention outputs from all heads\n","        # (batch_size, seq_len, d_model)\n","        attn_output = torch.cat(attn_outputs, dim=-1)\n","\n","        # Final linear transformation\n","        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n","        output = self.W_O(attn_output)\n","\n","        return output\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, d_model, h):\n","        super().__init__()\n","        self.attention = MultiHeadAttention(d_model, h)\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(d_model, d_model * 4),\n","            nn.ReLU(),\n","            nn.Linear(d_model * 4, d_model)\n","        )\n","        self.layer_norm1 = nn.LayerNorm(d_model)\n","        self.layer_norm2 = nn.LayerNorm(d_model)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, seq_len, d_model)\n","\n","        # Self-attention\n","        attention_output = self.attention(x)\n","\n","        # Add & Norm\n","        x = self.layer_norm1(x + attention_output)\n","\n","        # Feed-forward\n","        ff_output = self.feed_forward(x)\n","\n","        # Add & Norm\n","        x = self.layer_norm2(x + ff_output)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"otaVgmcqzhQ3"},"source":["## Positional Encoding\n","\n","This class implements positional encoding:\n","1. It creates a matrix of positional encodings.\n","2. The encodings are based on sine and cosine functions of different frequencies.\n","3. These encodings are added to the input embeddings to provide position information."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1720265917837,"user":{"displayName":"SY K","userId":"03708459796910042589"},"user_tz":-540},"id":"1ACDQnqlzk4t"},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=512):\n","        super().__init__()\n","\n","        # Create positional encoding matrix\n","        # Shape: (1, max_len, d_model)\n","        pe = torch.zeros(1, max_len, d_model)\n","        position = torch.arange(0, max_len).unsqueeze(1).float()\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n","\n","        pe[0, :, 0::2] = torch.sin(position * div_term)\n","        pe[0, :, 1::2] = torch.cos(position * div_term)\n","\n","        # Register pe as a buffer, which is a tensor that's part of the module\n","        # but not a parameter. This allows it to be saved and loaded with the model,\n","        # and moved to the correct device automatically.\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, seq_len, d_model)\n","        # Add positional encoding to the input\n","        return x + self.pe[:, :x.size(1)]"]},{"cell_type":"markdown","metadata":{"id":"965sKFkSzl88"},"source":["## Encoder\n","\n","This is the main Encoder class:\n","1. It embeds the input tokens and adds positional encoding.\n","2. The input is passed through multiple Transformer blocks.\n","3. The output is averaged across the sequence dimension.\n","4. A final linear layer produces class logits."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1720265917837,"user":{"displayName":"SY K","userId":"03708459796910042589"},"user_tz":-540},"id":"iuhcQrZfzpFu"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, vocab_size, d_model, num_layers, h, num_classes):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_encoding = PositionalEncoding(d_model)\n","        self.layers = nn.ModuleList([TransformerBlock(d_model, h) for _ in range(num_layers)])\n","        self.fc = nn.Linear(d_model, num_classes)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, seq_len)\n","\n","        # Embedding\n","        # (batch_size, seq_len) -> (batch_size, seq_len, d_model)\n","        x = self.embedding(x)\n","\n","        # Add positional encoding\n","        x = self.pos_encoding(x)\n","\n","        # Pass through transformer blocks\n","        for layer in self.layers:\n","            x = layer(x)\n","\n","        # Global average pooling\n","        # (batch_size, seq_len, d_model) -> (batch_size, d_model)\n","        x = x.mean(dim=1)\n","\n","        # Classification\n","        # (batch_size, d_model) -> (batch_size, num_classes)\n","        x = self.fc(x)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"bj1OG3rwzt_T"},"source":["## Data Preparation\n","\n","This section prepares the SST-2 dataset:\n","1. We use a pre-trained tokenizer from the 'distilbert-base-cased' model.\n","2. The SST-2 dataset is loaded using the `load_dataset` function.\n","3. A tokenization function is defined and applied to the dataset.\n","4. The dataset is formatted for PyTorch and split into training and validation sets."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["2498489493384935a769013262098879","fd272ae2647549ec8dc56a979ad93b11","538849d860f7463caddc10be7a05e93c","9be048f0801e4ca28cdcb968bde7196a","7fbaadb094394540a79fb74f8f242589","3998e849f3334542b4107b9e47c17e41","393c89c487674f3fb72551fde30726a8","50a3c7a1782f428fa0c44ddd2b458844","fa889d5844d447b680ca3dfa86e311dd","8dc15a393153448aaacc8923fc440d64","7c654ca834cf4b3faf0b9b5b3bb9eab4"]},"executionInfo":{"elapsed":2585,"status":"ok","timestamp":1720265920415,"user":{"displayName":"SY K","userId":"03708459796910042589"},"user_tz":-540},"id":"4K6sHZ6zzsj2","outputId":"5546eabc-8590-40d6-803c-6871f2d27b2f"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2498489493384935a769013262098879","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/872 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Load a pre-trained tokenizer. We use DistilBERT's tokenizer, which is a smaller, faster version of BERT\n","tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')\n","\n","# Load the SST-2 (Stanford Sentiment Treebank) dataset from the GLUE benchmark\n","# This dataset contains movie reviews labeled as positive or negative\n","raw_datasets = load_dataset(\"glue\", \"sst2\")\n","\n","# Define a function to tokenize our input sentences\n","def tokenize_function(examples):\n","    # Tokenize the sentences, add padding to the maximum length, and truncate if necessary\n","    # max_length=128 means we'll use sequences of up to 128 tokens\n","    return tokenizer(examples['sentence'], padding='max_length', truncation=True, max_length=128)\n","\n","# Apply the tokenization function to our dataset\n","# This processes the entire dataset, tokenizing each sentence\n","tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","\n","# Remove unnecessary columns from the dataset\n","# We only need the tokenized input and the label\n","tokenized_datasets = tokenized_datasets.remove_columns(['sentence', 'idx'])\n","\n","# Rename the 'label' column to 'labels' for consistency with PyTorch conventions\n","tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n","\n","# Set the format of the datasets to PyTorch tensors\n","tokenized_datasets.set_format('torch')\n","\n","# Create data loaders for training and evaluation\n","# DataLoader handles batching and shuffling of the data\n","train_dataloader = DataLoader(tokenized_datasets['train'], shuffle=True, batch_size=32)\n","eval_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=32)"]},{"cell_type":"markdown","metadata":{"id":"k6UT4ATYz0Rn"},"source":["## Model Initialization\n","\n","Here we initialize our Encoder model:\n","1. We check if a GPU is available and set the device accordingly.\n","2. The model is created with specific hyperparameters and moved to the appropriate device."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1720265920416,"user":{"displayName":"SY K","userId":"03708459796910042589"},"user_tz":-540},"id":"4ZxyYFiNzzkD"},"outputs":[],"source":["# Check if a GPU is available, and set the device accordingly\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Initialize our Encoder model with specific hyperparameters\n","model = Encoder(\n","    vocab_size=tokenizer.vocab_size,  # Size of the tokenizer's vocabulary\n","    d_model=256,  # Dimension of the model's hidden states\n","    num_layers=4,  # Number of transformer blocks\n","    h=8,  # Number of attention heads\n","    num_classes=2  # Number of output classes (positive/negative)\n",").to(device)  # Move the model to the appropriate device (GPU if available)"]},{"cell_type":"markdown","metadata":{"id":"AtKl9vqGz4Ub"},"source":["## Training Loop\n","\n","This section implements the training loop:\n","1. We define an Adam optimizer and set the number of epochs.\n","2. For each epoch, we iterate over the training data, compute loss, and update the model parameters.\n","3. After each epoch, we evaluate the model on the validation set and print the accuracy."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":554560,"status":"ok","timestamp":1720266474970,"user":{"displayName":"SY K","userId":"03708459796910042589"},"user_tz":-540},"id":"Iz68svY0z6m6","outputId":"49297575-6236-4067-859e-4307bd6c2141"},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1/5: 100%|██████████| 2105/2105 [01:48<00:00, 19.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/5, Average Loss: 0.4719\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 28/28 [00:00<00:00, 74.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 77.18%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/5: 100%|██████████| 2105/2105 [01:54<00:00, 18.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/5, Average Loss: 0.2657\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 28/28 [00:00<00:00, 72.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 80.62%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/5: 100%|██████████| 2105/2105 [01:48<00:00, 19.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/5, Average Loss: 0.1826\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 28/28 [00:00<00:00, 77.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 77.06%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/5: 100%|██████████| 2105/2105 [01:50<00:00, 19.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/5, Average Loss: 0.1291\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 28/28 [00:00<00:00, 78.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 80.85%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/5: 100%|██████████| 2105/2105 [01:50<00:00, 19.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/5, Average Loss: 0.0995\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 28/28 [00:00<00:00, 78.69it/s]"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 79.36%\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Initialize the Adam optimizer with a learning rate of 1e-4\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","num_epochs = 5  # Number of times to iterate over the entire dataset\n","\n","# Training Loop\n","for epoch in range(num_epochs):\n","    model.train()  # Set the model to training mode\n","    total_loss = 0\n","\n","    # Iterate over batches in the training data\n","    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n","        optimizer.zero_grad()  # Reset gradients\n","\n","        # Move input data to the appropriate device\n","        input_ids = batch['input_ids'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        outputs = model(input_ids)  # Forward pass\n","        loss = F.cross_entropy(outputs, labels)  # Compute loss\n","        loss.backward()  # Backpropagation\n","        optimizer.step()  # Update model parameters\n","\n","        total_loss += loss.item()  # Accumulate loss for reporting\n","\n","    # Compute and print average loss for the epoch\n","    avg_loss = total_loss / len(train_dataloader)\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n","\n","    # Evaluation\n","    model.eval()  # Set the model to evaluation mode\n","    correct = 0\n","    total = 0\n","\n","    # Disable gradient computation for evaluation\n","    with torch.no_grad():\n","        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","            input_ids = batch['input_ids'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            outputs = model(input_ids)  # Forward pass\n","            _, predicted = torch.max(outputs.data, 1)  # Get predicted class\n","\n","            total += labels.size(0)  # Count total number of samples\n","            correct += (predicted == labels).sum().item()  # Count correct predictions\n","\n","    # Compute and print accuracy\n","    accuracy = 100 * correct / total\n","    print(f\"Validation Accuracy: {accuracy:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"r3Cw0wTtz_nD"},"source":["## Model Testing\n","\n","Finally, we test our trained model:\n","1. We define a `predict_sentiment` function that takes a sentence and returns the predicted sentiment.\n","2. We test the model on a few example sentences and print the results.\n","\n","This completes our implementation of a Transformer-based sentiment analysis model. The model learns to classify movie reviews as positive or negative based on the text content."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1720266474971,"user":{"displayName":"SY K","userId":"03708459796910042589"},"user_tz":-540},"id":"wEgudszSz-5m","outputId":"8a01031d-955c-486f-8ff8-ea3949f49142"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Testing the model:\n","Sentence: 'This movie is great!'\n","Predicted sentiment: Positive\n","\n","Sentence: 'I didn't enjoy the film at all.'\n","Predicted sentiment: Positive\n","\n","Sentence: 'The acting was superb and the plot was engaging.'\n","Predicted sentiment: Positive\n","\n","Sentence: 'A complete waste of time and money.'\n","Predicted sentiment: Positive\n","\n"]}],"source":["def predict_sentiment(text):\n","    # Tokenize input text and move to appropriate device\n","    input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n","\n","    with torch.no_grad():  # Disable gradient computation\n","        output = model(input_ids)  # Forward pass\n","        prediction = torch.argmax(output, dim=1)  # Get predicted class\n","\n","    # Return \"Positive\" for class 1, \"Negative\" for class 0\n","    return \"Positive\" if prediction.item() == 1 else \"Negative\"\n","\n","# Test sentences to evaluate our model\n","test_sentences = [\n","    \"This movie is great!\",\n","    \"I didn't enjoy the film at all.\",\n","    \"The acting was superb and the plot was engaging.\",\n","    \"A complete waste of time and money.\"\n","]\n","\n","print(\"\\nTesting the model:\")\n","for sentence in test_sentences:\n","    sentiment = predict_sentiment(sentence)\n","    print(f\"Sentence: '{sentence}'\")\n","    print(f\"Predicted sentiment: {sentiment}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"PQ0aRrZy2_0b"},"source":["# Vectorized version"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":666,"referenced_widgets":["ea12351362c04dbe872d6c72f9bb41a4","05bd866484fe43159af58431f6700336","18cc311a4de44cdd838bc06a9ffcda20","6fa03144f53f4da1b5ba8ad2efc35c21","d243d5a935334be39e4c20897363f717","b00b6aeec7914f8f9f5339d51b3897d7","ad56dbfbb9904e3684be070e11c0571f","d7f62bf7579041adb99aa0c20eee608e","7c41dda1e24c4db6b3798880f36eec64","b3afd66a36b249e1abf067ef26390305","66750d08b7574609b3132281a979ec88"]},"executionInfo":{"elapsed":425704,"status":"ok","timestamp":1720266916290,"user":{"displayName":"SY K","userId":"03708459796910042589"},"user_tz":-540},"id":"E1odgkaR3CDh","outputId":"22a9da7b-91d7-4c0b-d4bd-bfc715d155b0"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea12351362c04dbe872d6c72f9bb41a4","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 1/5: 100%|██████████| 2105/2105 [01:25<00:00, 24.54it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/5, Average Loss: 0.4836\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 28/28 [00:00<00:00, 76.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 77.41%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/5: 100%|██████████| 2105/2105 [01:23<00:00, 25.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/5, Average Loss: 0.2672\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 28/28 [00:00<00:00, 70.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 79.47%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/5: 100%|██████████| 2105/2105 [01:24<00:00, 25.05it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/5, Average Loss: 0.1833\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 28/28 [00:00<00:00, 75.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 79.13%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/5: 100%|██████████| 2105/2105 [01:23<00:00, 25.16it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/5, Average Loss: 0.1308\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 28/28 [00:00<00:00, 75.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 78.67%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/5: 100%|██████████| 2105/2105 [01:23<00:00, 25.17it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/5, Average Loss: 0.0977\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 28/28 [00:00<00:00, 74.93it/s]"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 78.90%\n","\n","Testing the model:\n","Sentence: 'This movie is great!'\n","Predicted sentiment: Positive\n","\n","Sentence: 'I didn't enjoy the film at all.'\n","Predicted sentiment: Positive\n","\n","Sentence: 'The acting was superb and the plot was engaging.'\n","Predicted sentiment: Positive\n","\n","Sentence: 'A complete waste of time and money.'\n","Predicted sentiment: Negative\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","from transformers import AutoTokenizer\n","from datasets import load_dataset\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, h):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.h = h\n","        self.d_k = d_model // h\n","\n","        self.W_Q = nn.Linear(d_model, d_model)\n","        self.W_K = nn.Linear(d_model, d_model)\n","        self.W_V = nn.Linear(d_model, d_model)\n","        self.W_O = nn.Linear(d_model, d_model)\n","\n","    def forward(self, x):\n","        batch_size, seq_len, _ = x.size()\n","\n","        # Vectorized computation of Q, K, V for all heads at once\n","        Q = self.W_Q(x).view(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n","        K = self.W_K(x).view(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n","        V = self.W_V(x).view(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n","\n","        # Compute attention scores for all heads in parallel\n","        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n","\n","        # Apply softmax to get attention weights\n","        attn_weights = F.softmax(attn_scores, dim=-1)\n","\n","        # Compute attention output for all heads in parallel\n","        attn_output = torch.matmul(attn_weights, V)\n","\n","        # Reshape and apply final linear transformation\n","        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n","        return self.W_O(attn_output)\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, d_model, h):\n","        super().__init__()\n","        self.attention = MultiHeadAttention(d_model, h)\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(d_model, d_model * 4),\n","            nn.ReLU(),\n","            nn.Linear(d_model * 4, d_model)\n","        )\n","        self.layer_norm1 = nn.LayerNorm(d_model)\n","        self.layer_norm2 = nn.LayerNorm(d_model)\n","\n","    def forward(self, x):\n","        attention_output = self.attention(x)\n","        x = self.layer_norm1(x + attention_output)\n","        ff_output = self.feed_forward(x)\n","        x = self.layer_norm2(x + ff_output)\n","        return x\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=512):\n","        super().__init__()\n","        pe = torch.zeros(1, max_len, d_model)\n","        position = torch.arange(0, max_len).unsqueeze(1).float()\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n","        pe[0, :, 0::2] = torch.sin(position * div_term)\n","        pe[0, :, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1)]\n","\n","class Encoder(nn.Module):\n","    def __init__(self, vocab_size, d_model, num_layers, h, num_classes):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_encoding = PositionalEncoding(d_model)\n","        self.layers = nn.ModuleList([TransformerBlock(d_model, h) for _ in range(num_layers)])\n","        self.fc = nn.Linear(d_model, num_classes)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = self.pos_encoding(x)\n","        for layer in self.layers:\n","            x = layer(x)\n","        x = x.mean(dim=1)\n","        x = self.fc(x)\n","        return x\n","\n","tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')\n","raw_datasets = load_dataset(\"glue\", \"sst2\")\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples['sentence'], padding='max_length', truncation=True, max_length=128)\n","\n","tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","tokenized_datasets = tokenized_datasets.remove_columns(['sentence', 'idx'])\n","tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n","tokenized_datasets.set_format('torch')\n","\n","train_dataloader = DataLoader(tokenized_datasets['train'], shuffle=True, batch_size=32)\n","eval_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=32)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = Encoder(\n","    vocab_size=tokenizer.vocab_size,\n","    d_model=256,\n","    num_layers=4,\n","    h=8,\n","    num_classes=2\n",").to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","num_epochs = 5\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids)\n","        loss = F.cross_entropy(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(train_dataloader)\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n","\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","            input_ids = batch['input_ids'].to(device)\n","            labels = batch['labels'].to(device)\n","            outputs = model(input_ids)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    print(f\"Validation Accuracy: {accuracy:.2f}%\")\n","\n","def predict_sentiment(text):\n","    input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n","    with torch.no_grad():\n","        output = model(input_ids)\n","        prediction = torch.argmax(output, dim=1)\n","    return \"Positive\" if prediction.item() == 1 else \"Negative\"\n","\n","test_sentences = [\n","    \"This movie is great!\",\n","    \"I didn't enjoy the film at all.\",\n","    \"The acting was superb and the plot was engaging.\",\n","    \"A complete waste of time and money.\"\n","]\n","\n","print(\"\\nTesting the model:\")\n","for sentence in test_sentences:\n","    sentiment = predict_sentiment(sentence)\n","    print(f\"Sentence: '{sentence}'\")\n","    print(f\"Predicted sentiment: {sentiment}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"maVYKiUb3DI7"},"source":["# Using Pytorch Modules"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":468272,"status":"ok","timestamp":1720267384543,"user":{"displayName":"SY K","userId":"03708459796910042589"},"user_tz":-540},"id":"FLPNa3w33BsR","outputId":"524455bb-3ac1-473e-f96d-d49df465aab5"},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1/5: 100%|██████████| 2105/2105 [01:33<00:00, 22.58it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/5, Average Loss: 0.5441\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 28/28 [00:00<00:00, 83.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 77.18%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/5: 100%|██████████| 2105/2105 [01:32<00:00, 22.71it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/5, Average Loss: 0.3762\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 28/28 [00:00<00:00, 80.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 78.67%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/5: 100%|██████████| 2105/2105 [01:32<00:00, 22.70it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/5, Average Loss: 0.3018\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 28/28 [00:00<00:00, 86.38it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 79.93%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/5: 100%|██████████| 2105/2105 [01:32<00:00, 22.70it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/5, Average Loss: 0.2510\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 28/28 [00:00<00:00, 82.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 80.28%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/5: 100%|██████████| 2105/2105 [01:32<00:00, 22.69it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/5, Average Loss: 0.2153\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 28/28 [00:00<00:00, 88.28it/s]"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 79.70%\n","\n","Testing the model:\n","Sentence: 'This movie is great!'\n","Predicted sentiment: Positive\n","\n","Sentence: 'I didn't enjoy the film at all.'\n","Predicted sentiment: Positive\n","\n","Sentence: 'The acting was superb and the plot was engaging.'\n","Predicted sentiment: Positive\n","\n","Sentence: 'A complete waste of time and money.'\n","Predicted sentiment: Negative\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import AutoTokenizer\n","from datasets import load_dataset\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super().__init__()\n","        self.multihead_attn = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n","\n","    def forward(self, x):\n","        return self.multihead_attn(x, x, x)[0]\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super().__init__()\n","        self.attention = MultiHeadAttention(d_model, num_heads)\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(d_model, d_model * 4),\n","            nn.ReLU(),\n","            nn.Linear(d_model * 4, d_model)\n","        )\n","        self.layer_norm1 = nn.LayerNorm(d_model)\n","        self.layer_norm2 = nn.LayerNorm(d_model)\n","\n","    def forward(self, x):\n","        x = self.layer_norm1(x + self.attention(x))\n","        x = self.layer_norm2(x + self.feed_forward(x))\n","        return x\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=512):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=0.1)\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n","        pe = torch.zeros(1, max_len, d_model)\n","        pe[0, :, 0::2] = torch.sin(position * div_term)\n","        pe[0, :, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1)]\n","        return self.dropout(x)\n","\n","class Encoder(nn.Module):\n","    def __init__(self, vocab_size, d_model, num_layers, num_heads, num_classes):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_encoding = PositionalEncoding(d_model)\n","        encoder_layer = nn.TransformerEncoderLayer(d_model, num_heads, dim_feedforward=d_model * 4, batch_first=True)\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n","        self.fc = nn.Linear(d_model, num_classes)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = self.pos_encoding(x)\n","        x = self.transformer_encoder(x)\n","        x = x.mean(dim=1)\n","        x = self.fc(x)\n","        return x\n","\n","# Data preparation\n","tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')\n","raw_datasets = load_dataset(\"glue\", \"sst2\")\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples['sentence'], padding='max_length', truncation=True, max_length=128)\n","\n","tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","tokenized_datasets = tokenized_datasets.remove_columns(['sentence', 'idx'])\n","tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n","tokenized_datasets.set_format('torch')\n","\n","train_dataloader = DataLoader(tokenized_datasets['train'], shuffle=True, batch_size=32)\n","eval_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=32)\n","\n","# Model initialization\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = Encoder(\n","    vocab_size=tokenizer.vocab_size,\n","    d_model=256,\n","    num_layers=4,\n","    num_heads=8,\n","    num_classes=2\n",").to(device)\n","\n","# Training settings\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","num_epochs = 5\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids)\n","        loss = F.cross_entropy(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(train_dataloader)\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n","\n","    # Evaluation\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","            input_ids = batch['input_ids'].to(device)\n","            labels = batch['labels'].to(device)\n","            outputs = model(input_ids)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    print(f\"Validation Accuracy: {accuracy:.2f}%\")\n","\n","# Model testing\n","def predict_sentiment(text):\n","    input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n","    with torch.no_grad():\n","        output = model(input_ids)\n","        prediction = torch.argmax(output, dim=1)\n","    return \"Positive\" if prediction.item() == 1 else \"Negative\"\n","\n","test_sentences = [\n","    \"This movie is great!\",\n","    \"I didn't enjoy the film at all.\",\n","    \"The acting was superb and the plot was engaging.\",\n","    \"A complete waste of time and money.\"\n","]\n","\n","print(\"\\nTesting the model:\")\n","for sentence in test_sentences:\n","    sentiment = predict_sentiment(sentence)\n","    print(f\"Sentence: '{sentence}'\")\n","    print(f\"Predicted sentiment: {sentiment}\\n\")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNmzuKkqFDiS8WWvvRltQnk","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"05bd866484fe43159af58431f6700336":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b00b6aeec7914f8f9f5339d51b3897d7","placeholder":"​","style":"IPY_MODEL_ad56dbfbb9904e3684be070e11c0571f","value":"Map: 100%"}},"18cc311a4de44cdd838bc06a9ffcda20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7f62bf7579041adb99aa0c20eee608e","max":1821,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7c41dda1e24c4db6b3798880f36eec64","value":1821}},"2498489493384935a769013262098879":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fd272ae2647549ec8dc56a979ad93b11","IPY_MODEL_538849d860f7463caddc10be7a05e93c","IPY_MODEL_9be048f0801e4ca28cdcb968bde7196a"],"layout":"IPY_MODEL_7fbaadb094394540a79fb74f8f242589"}},"393c89c487674f3fb72551fde30726a8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3998e849f3334542b4107b9e47c17e41":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50a3c7a1782f428fa0c44ddd2b458844":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"538849d860f7463caddc10be7a05e93c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_50a3c7a1782f428fa0c44ddd2b458844","max":872,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fa889d5844d447b680ca3dfa86e311dd","value":872}},"66750d08b7574609b3132281a979ec88":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fa03144f53f4da1b5ba8ad2efc35c21":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3afd66a36b249e1abf067ef26390305","placeholder":"​","style":"IPY_MODEL_66750d08b7574609b3132281a979ec88","value":" 1821/1821 [00:00&lt;00:00, 7055.05 examples/s]"}},"7c41dda1e24c4db6b3798880f36eec64":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7c654ca834cf4b3faf0b9b5b3bb9eab4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7fbaadb094394540a79fb74f8f242589":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dc15a393153448aaacc8923fc440d64":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9be048f0801e4ca28cdcb968bde7196a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8dc15a393153448aaacc8923fc440d64","placeholder":"​","style":"IPY_MODEL_7c654ca834cf4b3faf0b9b5b3bb9eab4","value":" 872/872 [00:00&lt;00:00, 7012.26 examples/s]"}},"ad56dbfbb9904e3684be070e11c0571f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b00b6aeec7914f8f9f5339d51b3897d7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3afd66a36b249e1abf067ef26390305":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d243d5a935334be39e4c20897363f717":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7f62bf7579041adb99aa0c20eee608e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea12351362c04dbe872d6c72f9bb41a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_05bd866484fe43159af58431f6700336","IPY_MODEL_18cc311a4de44cdd838bc06a9ffcda20","IPY_MODEL_6fa03144f53f4da1b5ba8ad2efc35c21"],"layout":"IPY_MODEL_d243d5a935334be39e4c20897363f717"}},"fa889d5844d447b680ca3dfa86e311dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fd272ae2647549ec8dc56a979ad93b11":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3998e849f3334542b4107b9e47c17e41","placeholder":"​","style":"IPY_MODEL_393c89c487674f3fb72551fde30726a8","value":"Map: 100%"}}}}},"nbformat":4,"nbformat_minor":0}
