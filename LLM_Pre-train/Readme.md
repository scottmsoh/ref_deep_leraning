

Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length:</br>
This paper introduces Megalodon, a neural architecture designed to efficiently model long sequences. Megalodon employs novel techniques such as complex exponential moving averages (CEMA) and timestep normalization to enhance pre-training efficiency and stability. These methods aim to improve handling of long-context sequences, which is crucial for tasks requiring extensive context comprehension​ (ar5iv)​.</br>
https://ar5iv.labs.arxiv.org/html/2404.08801

Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models:</br>
This research investigates the potential of pre-training checkpoints to enhance the trustworthiness of LLMs. By analyzing activations during pre-training, the study identifies layers where model representations are linearly separable and can be used to improve model trustworthiness through activation intervention techniques​ (ar5iv)​.</br>
https://ar5iv.labs.arxiv.org/html/2402.19465

Simple and Scalable Strategies to Continually Pre-train Large Language Models:</br>
This work explores efficient methods for continuously updating LLMs with new data. By using strategies such as learning rate rewarming and including a fraction of previous training data, the paper addresses issues like catastrophic forgetting, enabling LLMs to maintain performance while integrating new information​ (Sebastian Raschka, PhD)​​ (ar5iv)​.</br>
https://sebastianraschka.com/blog/2024/research-papers-in-march-2024.html

NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data:</br>
NuNER focuses on pre-training a language representation model for entity recognition using data annotated by LLMs. The approach leverages a diverse set of 200k unique concepts, significantly broader than traditional NER datasets, and employs a contrastive learning framework to align text embeddings with concept embeddings​ (ar5iv)​.</br>
https://ar5iv.labs.arxiv.org/html/2402.15343
