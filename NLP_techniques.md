

Tokenizer</br>
Pre-training and Fine-tuning:</br>
- Tokenization is a crucial step that occurs in both pre-training and fine-tuning phases.</br>
During pre-training, the tokenizer is used to convert large-scale text corpora into tokens that the model can learn from.</br> 
During fine-tuning, the same or a similar tokenizer is applied to the task-specific dataset to prepare it for the model </br>
that has already been pre-trained.</br>


## Pre-training



## Fine-tune
