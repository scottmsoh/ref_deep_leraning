

## Human Evaluation of LLM Responses

This repository contains an evaluation of large language model (LLM) responses based on human preferences.</br>
The evaluation focuses on the appropriateness of responses generated by models like GPT-4 and Gemini.</br>

### 2. Human Evaluation

### Methodology

- **Human-Based Evaluation**:
  - Responses from the models are evaluated based on human preferences.</br>
  - This evaluation method is considered more accurate as it relies on human judgment.</br>

- **Typical Evaluation Method**:
  - **Chatbot Arena** and similar evaluation platforms are used.</br>
  - **Pairwise Comparison**: Two responses are compared, and the preferred response is selected.</br>
  - **Elo Rating**: Commonly used in competitive games, the Elo rating system ranks models by assigning scores based on pairwise comparison results.</br>

### Challenges

- **Bias and Variability**:
  - Model templates or response styles can introduce biases, affecting anonymity and leading to variable results.</br>
  - This method can be time-consuming and costly.</br>

- **Representative Key Questions**:
  - Key questions are sampled, and human evaluators score the responses anonymously.</br>

### Expert Evaluation
- **Expert-Level Evaluation**:
  - In specific domains, expert-level evaluators are required, which can be labor-intensive and time-consuming.</br>

